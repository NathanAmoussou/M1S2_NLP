{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42ef07a",
   "metadata": {},
   "source": [
    "# Finance SQL Database Interrogation in Natural Language\n",
    "\n",
    "This project is more of an exercise prompted by NLP class of Professor E. Cabrio of Université Côte d'Azur. Here, we aim to provide a symbolic (and an attempted machine learning) approach to go from a natural language request to a properly parsed SQL request (tailored to our specific finance-oriented database).\n",
    "\n",
    "**Author:** Nathan Amoussou, Tristan Patout\\\n",
    "**Course:** Natural Language Processing of Master 1 Informatique of Université Côte d'Azur\\\n",
    "**Date:** 04/20/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403edc7",
   "metadata": {},
   "source": [
    "## 1. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1c8f9",
   "metadata": {},
   "source": [
    "Import necessary libraries and define global configurations like file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef15bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use %%capture to hide installation outputs if desired\n",
    "# %%capture\n",
    "# %pip install pandas spacy -q\n",
    "# !python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy # For optional visualizations\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import warnings \n",
    "\n",
    "# Filter specific warnings if needed\n",
    "# warnings.filterwarnings(\"ignore\", message=r\".*\\[W036\\].*\") \n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "sp500_file = 'Database_ressources/sp_500_companies_with_financial_information.csv'\n",
    "marketcap_file = 'Database_ressources/top_global_companies_by_market_cap.csv'\n",
    "db_file = 'companies_database.db'\n",
    "table_name = 'companies'\n",
    "\n",
    "print(f\"Database file set to: {db_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd3c05",
   "metadata": {},
   "source": [
    "## 2. Database Creation & Population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99341d1c",
   "metadata": {},
   "source": [
    "Load raw data from CSV files into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "try:\n",
    "    df_sp500 = pd.read_csv(sp500_file)\n",
    "    df_marketcap = pd.read_csv(marketcap_file)\n",
    "    print(\"CSV files loaded successfully.\")\n",
    "    # Optional: Display head/info\n",
    "    # print(\"S&P 500 Head:\\n\", df_sp500.head())\n",
    "    # print(\"\\nMarket Cap Head:\\n\", df_marketcap.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    # Handle error appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da39f1",
   "metadata": {},
   "source": [
    "Clean and prepare the DataFrames: select relevant columns, rename columns for consistency, clean numeric/date values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare S&P 500 DataFrame (df1)\n",
    "df1 = df_sp500[['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry', 'Founded']].copy()\n",
    "df1.rename(columns={'GICS Sector': 'Sector', 'GICS Sub-Industry': 'Industry'}, inplace=True)\n",
    "df1['Founded'] = df1['Founded'].astype(str).str.extract(r'(\\d{4})', expand=False)\n",
    "print(\"S&P 500 DataFrame prepared.\")\n",
    "\n",
    "# Prepare Market Cap DataFrame (df2)\n",
    "df2 = df_marketcap[['Company Code', 'Marketcap', 'Stock Price', 'Country']].copy()\n",
    "df2.rename(columns={'Company Code': 'Symbol', 'Stock Price': 'Stockprice'}, inplace=True)\n",
    "# --- Cleaning functions (define or copy here) ---\n",
    "def clean_marketcap(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    if not isinstance(value, str):\n",
    "        return None\n",
    "    value = value.replace('$', '').replace(',', '').strip()\n",
    "    if 'T' in value:\n",
    "        # Handle potential spaces like '3.033 T'\n",
    "        return float(value.replace('T', '').strip()) * 1e12\n",
    "    elif 'B' in value:\n",
    "        return float(value.replace('B', '').strip()) * 1e9\n",
    "    elif 'M' in value:\n",
    "        return float(value.replace('M', '').strip()) * 1e6\n",
    "    try:\n",
    "        # Attempt direct conversion after basic cleaning\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None # Return None if conversion still fails\n",
    "def clean_stockprice(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    if not isinstance(value, str):\n",
    "        return None\n",
    "    # Remove '$' and ',' before converting\n",
    "    value = value.replace('$', '').replace(',', '').strip()\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None # Return None if conversion fails\n",
    "df2['Marketcap'] = df2['Marketcap'].apply(clean_marketcap)\n",
    "df2['Stockprice'] = df2['Stockprice'].apply(clean_stockprice)\n",
    "print(\"Market Cap DataFrame prepared and cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d3b5d",
   "metadata": {},
   "source": [
    "Merge the prepared DataFrames and select the final columns for the database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c301d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "merged_df = pd.merge(df1, df2, on='Symbol', how='inner')\n",
    "merged_df = merged_df.sort_values('Marketcap', ascending=False).drop_duplicates('Symbol', keep='first')\n",
    "print(f\"DataFrames merged. Result shape: {merged_df.shape}\")\n",
    "\n",
    "# Prepare Final DataFrame for SQL\n",
    "final_columns = ['Symbol', 'Security', 'Sector', 'Industry', 'Founded', 'Marketcap', 'Stockprice', 'Country']\n",
    "# Ensure columns exist and select/reorder\n",
    "missing_cols = [col for col in final_columns if col not in merged_df.columns]\n",
    "if missing_cols: print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "final_df = merged_df[[col for col in final_columns if col in merged_df.columns]].copy() \n",
    "print(\"Final DataFrame prepared for SQL.\")\n",
    "# print(final_df.head())\n",
    "# final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed6c07",
   "metadata": {},
   "source": [
    "Create the SQLite database and table, then populate it with the final DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f52ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to SQLite\n",
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "    # Use pandas to_sql to create/replace table and insert data\n",
    "    final_df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    print(f\"Data successfully imported into table '{table_name}' in '{db_file}'\")\n",
    "    \n",
    "    # Verification query\n",
    "    verify_df = pd.read_sql(f\"SELECT COUNT(*) as count FROM {table_name}\", conn)\n",
    "    print(f\"Verification: Found {verify_df['count'][0]} rows in the SQL table.\")\n",
    "    \n",
    "except sqlite3.Error as e:\n",
    "    print(f\"SQLite error during import: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during SQL import: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be789c19",
   "metadata": {},
   "source": [
    "## 3. NLP Pipeline Setup & Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57657f",
   "metadata": {},
   "source": [
    "### 3.1. Setup: NER Enhancement Helpers & Configuration\n",
    "Define functions for loading gazetteers, mappings, and setup for PhraseMatcher and EntityRuler patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gazetteer Loading Function ---\n",
    "def load_terms_from_db(db_path, table, column_name):\n",
    "    terms = set(); original_casing_terms = set()\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        query = f'SELECT DISTINCT \"{column_name}\" FROM \"{table}\" WHERE \"{column_name}\" IS NOT NULL AND \"{column_name}\" != \\'\\''\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        if column_name in df.columns:\n",
    "            original_casing_terms.update(term.strip() for term in df[column_name].astype(str) if term.strip())\n",
    "            terms.update(term.lower().strip() for term in df[column_name].astype(str) if term.strip())\n",
    "        else: print(f\"Warning: Column '{column_name}' not found in table '{table}'.\")\n",
    "    except Exception as e: print(f\"Error loading terms for '{column_name}': {e}\")\n",
    "    finally:\n",
    "        if conn: conn.close()\n",
    "    terms.discard(''); original_casing_terms.discard('')\n",
    "    print(f\"Loaded {len(terms)} unique lowercase terms for '{column_name}'.\")\n",
    "    return list(terms), list(original_casing_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Mappings ---\n",
    "country_mapping = {\n",
    "    \"american\": \"usa\", \"us\": \"usa\", \"u.s.\": \"usa\", \"u.s.a\": \"usa\",\n",
    "    \"uk\": \"united kingdom\", \"u.k.\": \"united kingdom\",\n",
    "    \"french\": \"france\", \"german\": \"germany\", \"spanish\": \"spain\",\n",
    "    \"indian\": \"india\"\n",
    "}\n",
    "sector_alias_mapping = {\n",
    "    \"it\": \"information technology\", \"info tech\": \"information technology\",\n",
    "    \"health\": \"health care\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ebfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Entity Ruler Pattern Definitions (Define patterns list) ---\n",
    "# Build the `patterns` list\n",
    "# (Using separate lists for countries, aliases, custom rules, etc., then combining)\n",
    "patterns = []\n",
    "\n",
    "# Custom patterns (Monetary, Ranking, Ops, Keywords, Cardinal) - Revised v4\n",
    "custom_patterns = [\n",
    "    # Monetary Values (Keep previous version - seemed okay for '$500 billion')\n",
    "    {\"label\": \"MONEY_VALUE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"LOWER\": {\"IN\": [\"b\", \"bn\", \"billion\", \"m\", \"mn\", \"million\", \"t\", \"tn\", \"trillion\"]}}, {\"IS_PUNCT\": True, \"OP\": \"?\"}]},\n",
    "    {\"label\": \"MONEY_VALUE\", \"pattern\": [{\"TEXT\": \"$\", \"OP\": \"?\"}, {\"LIKE_NUM\": True}, {\"LOWER\": {\"IN\": [\"b\", \"bn\", \"billion\", \"m\", \"mn\", \"million\", \"t\", \"tn\", \"trillion\"]}}, {\"IS_PUNCT\": True, \"OP\": \"?\"}]},\n",
    "    # Pattern for simple money amounts like $500 (Needs PRICE_VALUE label?)\n",
    "    {\"label\": \"PRICE_VALUE\", \"pattern\": [{\"TEXT\": \"$\", \"OP\": \"?\"}, {\"LIKE_NUM\": True}]}, # Label specifically for price context? Or reuse MONEY_VALUE? Let's try PRICE_VALUE first.\n",
    "\n",
    "    # Cardinal Number (for limits, but avoid matching years if possible)\n",
    "    {\"label\": \"CARDINAL\", \"pattern\": [{\"POS\": \"NUM\", \"ENT_TYPE\": \"\", \"SHAPE\": {\"NOT_IN\": [\"dddd\"]}}]}, # Avoid tagging 4-digit numbers as CARDINAL initially\n",
    "    {\"label\": \"YEAR_NUMBER\", \"pattern\": [{\"SHAPE\": \"dddd\", \"POS\": \"NUM\"}]}, # Specific pattern for years\n",
    "\n",
    "    # Ranking (Keep as is)\n",
    "    {\"label\": \"RANKING_MODIFIER\", \"pattern\": [{\"LOWER\": {\"IN\": [\"top\", \"most\", \"highest\", \"largest\", \"biggest\", \"least\", \"lowest\", \"smallest\"]}}]},\n",
    "    \n",
    "    # Comparison Ops (Add before/after)\n",
    "    {\"label\": \"COMPARISON_OP\", \"pattern\": [{\"LOWER\": {\"IN\": [\"over\", \"above\", \"under\", \"below\", \"after\", \"before\"]}}]},\n",
    "    {\"label\": \"COMPARISON_OP\", \"pattern\": [{\"LOWER\": \"more\"}, {\"LOWER\": \"than\"}]},\n",
    "    {\"label\": \"COMPARISON_OP\", \"pattern\": [{\"LOWER\": \"greater\"}, {\"LOWER\": \"than\"}]},\n",
    "    {\"label\": \"COMPARISON_OP\", \"pattern\": [{\"LOWER\": \"less\"}, {\"LOWER\": \"than\"}]},\n",
    "    {\"label\": \"COMPARISON_OP\", \"pattern\": [{\"TEXT\": {\"IN\": [\">\", \"<\"]}}]},\n",
    "\n",
    "    # Value Keywords (Keep as is)\n",
    "    {\"label\": \"VALUE_KEYWORD\", \"pattern\": [{\"LOWER\": {\"IN\": [\"valued\", \"value\", \"marketcap\", \"worth\"]}}]},\n",
    "    {\"label\": \"VALUE_KEYWORD\", \"pattern\": [{\"LOWER\": \"market\"}, {\"LOWER\": \"cap\"}]},\n",
    "\n",
    "    # Column Keywords (Add variations)\n",
    "    {\"label\": \"COLUMN_SECTOR\", \"pattern\": [{\"LOWER\": {\"IN\": [\"sector\", \"sectors\"]}}]},\n",
    "    {\"label\": \"COLUMN_INDUSTRY\", \"pattern\": [{\"LOWER\": {\"IN\": [\"industry\", \"industries\"]}}]},\n",
    "    {\"label\": \"COLUMN_COUNTRY\", \"pattern\": [{\"LOWER\": {\"IN\": [\"country\", \"countries\", \"location\", \"region\"]}}]}, # Added synonyms\n",
    "    {\"label\": \"COLUMN_FOUNDED\", \"pattern\": [{\"LOWER\": \"founded\"}]},\n",
    "    {\"label\": \"COLUMN_FOUNDED\", \"pattern\": [{\"LOWER\": \"founding\"}, {\"LOWER\": \"date\"}]}, # Added variation\n",
    "    {\"label\": \"COLUMN_STOCKPRICE\", \"pattern\": [{\"LOWER\": {\"IN\": [\"stock\", \"price\", \"stockprice\", \"stockprices\"]}}]}, # Added plural\n",
    "    {\"label\": \"COLUMN_STOCKPRICE\", \"pattern\": [{\"LOWER\": \"stock\"}, {\"LOWER\": \"price\"}]}, # Ensure multi-word matches\n",
    "    {\"label\": \"COLUMN_STOCKPRICE\", \"pattern\": [{\"LOWER\": \"stock\"}, {\"LOWER\": \"prices\"}]}, \n",
    "\n",
    "    # Ordering Keyword\n",
    "    {\"label\": \"ORDERING_KEYWORD\", \"pattern\": [{\"LOWER\": \"ordered\"}, {\"LOWER\": \"by\"}]},\n",
    "    {\"label\": \"ORDERING_KEYWORD\", \"pattern\": [{\"LOWER\": \"sorted\"}, {\"LOWER\": \"by\"}]},\n",
    "]\n",
    "print(f\"Defined {len(patterns)} patterns for EntityRuler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21878627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Placeholder for loaded gazetteers (will be loaded before testing) ---\n",
    "lc_sectors, unique_sectors_orig = [], []\n",
    "lc_countries, unique_countries_orig = [], []\n",
    "lc_industries, unique_industries_orig = [], []\n",
    "lc_companies, unique_companies_orig = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b184e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load spaCy Model ---\n",
    "nlp = None\n",
    "try:\n",
    "    # Consider loading a potentially larger model if accuracy is needed later\n",
    "    nlp = spacy.load(\"en_core_web_sm\") \n",
    "    print(\"Loaded 'en_core_web_sm' spaCy model.\")\n",
    "    # Clean ruler if it exists from previous runs\n",
    "    if \"entity_ruler\" in nlp.pipe_names: nlp.remove_pipe(\"entity_ruler\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading spaCy model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup PhraseMatcher ---\n",
    "matcher = None\n",
    "if nlp:\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr='LOWER') \n",
    "    print(\"PhraseMatcher initialized.\")\n",
    "else:\n",
    "    print(\"Warning: spaCy model not loaded, PhraseMatcher not initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup EntityRuler ---\n",
    "ruler = None\n",
    "if nlp:\n",
    "    if \"entity_ruler\" in nlp.pipe_names: nlp.remove_pipe(\"entity_ruler\") # Ensure clean state\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True}, before=\"ner\")\n",
    "    print(\"EntityRuler added to pipeline.\")\n",
    "else:\n",
    "     print(\"Warning: spaCy model not loaded, EntityRuler not added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb98eb7",
   "metadata": {},
   "source": [
    "### 3.2. Query Parsing Function & Helpers\n",
    "Define helper functions for parsing values and the main `parse_query_structure_refactored` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a98650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def parse_monetary_value(text_value):\n",
    "    \"\"\"Converts text like '1B', '$500 million', '1.5T', '$500' to a number.\"\"\"\n",
    "    if text_value is None: return None\n",
    "    \n",
    "    text_value = str(text_value).lower().replace('$', '').replace(',', '').strip()\n",
    "    multiplier = 1.0 \n",
    "\n",
    "    # Check for multipliers first\n",
    "    if 't' in text_value: \n",
    "        multiplier = 1e12\n",
    "        text_value = text_value.replace('trillion', '').replace('tn', '').replace('t', '')\n",
    "    elif 'b' in text_value: \n",
    "        multiplier = 1e9\n",
    "        text_value = text_value.replace('billion', '').replace('bn', '').replace('b', '')\n",
    "    elif 'm' in text_value: \n",
    "        multiplier = 1e6\n",
    "        text_value = text_value.replace('million', '').replace('mn', '').replace('m', '')\n",
    "    \n",
    "    # Now extract the numeric part using regex\n",
    "    try:\n",
    "        # Regex to find float or integer part, ignoring trailing non-numeric chars\n",
    "        numeric_part = re.findall(r\"^[-+]?\\d*\\.?\\d+\", text_value.strip())\n",
    "        if numeric_part:\n",
    "            return float(numeric_part[0]) * multiplier \n",
    "        else: \n",
    "            # print(f\"DEBUG: No numeric part found in '{text_value}' after cleaning.\")\n",
    "            return None # No number found\n",
    "    except (ValueError, TypeError, IndexError):\n",
    "        # print(f\"DEBUG: Error converting '{text_value}' to float.\")\n",
    "        return None # Conversion failed\n",
    "\n",
    "def map_comparison_operator(op_text):\n",
    "    \"\"\"Maps textual comparison operators to SQL operators.\"\"\"\n",
    "    op_text = op_text.lower().strip()\n",
    "    \n",
    "    # Direct mappings\n",
    "    if op_text in [\"over\", \"above\", \"greater than\", \"more than\", \">\", \"after\"]:\n",
    "        return \">\"\n",
    "    elif op_text in [\"under\", \"below\", \"less than\", \"<\", \"before\"]:\n",
    "        return \"<\"\n",
    "    elif op_text in [\"equal to\", \"equals\", \"is equal to\"]:\n",
    "        return \"=\"\n",
    "    elif op_text in [\"at least\", \"at most\"]:\n",
    "        return \">=\" if \"least\" in op_text else \"<=\"\n",
    "    return None\n",
    "    \n",
    "def normalize_term(term, original_list, alias_mapping=None):\n",
    "    \"\"\"Normalizes a found term against a list of canonical terms (original casing) and optional aliases.\"\"\"\n",
    "    term_lower = term.lower().strip()\n",
    "    \n",
    "    # 1. Check alias mapping first\n",
    "    if alias_mapping and term_lower in alias_mapping:\n",
    "        canonical_lower = alias_mapping[term_lower]\n",
    "        # Find original casing for the canonical name\n",
    "        for orig_term in original_list:\n",
    "            if orig_term.lower() == canonical_lower:\n",
    "                return orig_term\n",
    "        return canonical_lower # Return lowercase canonical if original casing not found (shouldn't happen ideally)\n",
    "\n",
    "    # 2. Check direct match (case-insensitive) against original list\n",
    "    for orig_term in original_list:\n",
    "        if orig_term.lower() == term_lower:\n",
    "            return orig_term # Return the original casing\n",
    "\n",
    "    # 3. Return original term if no match/normalization found\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29989cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Column Map ---\n",
    "COLUMN_MAP = {\n",
    "    \"COMPANY_NAME\": \"Security\", \"ORG\": \"Security\", \n",
    "    \"SECTOR_TERM\": \"Sector\", \"INDUSTRY_TERM\": \"Industry\",\n",
    "    \"COUNTRY_TERM\": \"Country\", \"GPE\": \"Country\",\n",
    "    \"VALUE_KEYWORD\": \"Marketcap\", \"COLUMN_SECTOR\": \"Sector\",\n",
    "    \"COLUMN_INDUSTRY\": \"Industry\", \"COLUMN_COUNTRY\": \"Country\",\n",
    "    \"COLUMN_FOUNDED\": \"Founded\", \"COLUMN_STOCKPRICE\": \"Stockprice\",\n",
    "    \"MONEY_VALUE\": \"Marketcap\", # Default column for monetary values\n",
    "    \"CARDINAL\": None # Cardinal numbers usually represent LIMIT, not a filter column\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8051df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Parsing Function ---\n",
    "def parse_query_structure_refactored(doc, identified_items, \n",
    "                                     orig_sectors, orig_industries, orig_countries): \n",
    "    \"\"\"\n",
    "    Parses the spaCy Doc and identified entities/terms. (Refactored v3 - Fixes)\n",
    "    \"\"\"\n",
    "    parsed_structure = {\n",
    "        'intent': None, 'select_cols': [], 'filters': [], 'limit': None,\n",
    "        'order_by': None, 'distinct': False, 'errors': []\n",
    "    }\n",
    "    lc_sectors_set = {s.lower() for s in orig_sectors}\n",
    "    lc_industries_set = {i.lower() for i in orig_industries}\n",
    "    lc_countries_set = {c.lower() for c in orig_countries}\n",
    "\n",
    "    items_by_label = {}\n",
    "    for item in identified_items:\n",
    "        label = item['label']\n",
    "        if label not in items_by_label: items_by_label[label] = []\n",
    "        items_by_label[label].append(item)\n",
    "        \n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_punct | token.is_stop]\n",
    "    root_verb = next((token for token in doc if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\"), None)\n",
    "    root_lemma = root_verb.lemma_.lower() if root_verb else None\n",
    "\n",
    "    # --- Define Flags ---\n",
    "    has_ranking_modifier = bool(items_by_label.get(\"RANKING_MODIFIER\"))\n",
    "    has_cardinal = bool(items_by_label.get(\"CARDINAL\"))\n",
    "    has_company = bool(items_by_label.get(\"COMPANY_NAME\") or items_by_label.get(\"ORG\"))\n",
    "    has_comparison = bool(items_by_label.get(\"COMPARISON_OP\"))\n",
    "    has_money = bool(items_by_label.get(\"MONEY_VALUE\"))\n",
    "    has_price = bool(items_by_label.get(\"PRICE_VALUE\")) \n",
    "    has_year = bool(items_by_label.get(\"YEAR_NUMBER\")) \n",
    "    has_date_keyword = bool(items_by_label.get(\"COLUMN_FOUNDED\")) \n",
    "    has_price_keyword = bool(items_by_label.get(\"COLUMN_STOCKPRICE\")) \n",
    "    has_ordering_keyword = bool(items_by_label.get(\"ORDERING_KEYWORD\")) \n",
    "    \n",
    "    mentioned_columns_labels = [lbl for lbl in items_by_label if lbl.startswith(\"COLUMN_\")]\n",
    "    # Refined condition for specific column request\n",
    "    is_specific_col_req = (root_lemma in [\"what\", \"show\", \"list\", \"tell\", \"give\"] and \\\n",
    "                           len(mentioned_columns_labels) == 1 and has_company and \\\n",
    "                           not has_ranking_modifier) # Added check to avoid conflict with ranking\n",
    "                           \n",
    "    is_list_all_keyword = (root_lemma in [\"list\", \"show\"] and any(l in lemmas for l in [\"all\", \"available\"]) and \\\n",
    "                           any(lbl in mentioned_columns_labels for lbl in [\"COLUMN_SECTOR\", \"COLUMN_INDUSTRY\", \"COLUMN_COUNTRY\"]))\n",
    "\n",
    "    # --- Refined Intent Prioritization ---\n",
    "    \n",
    "    # 1. List distinct column values (Specific intent)\n",
    "    if is_list_all_keyword:\n",
    "        parsed_structure['intent'] = 'list_values'\n",
    "        parsed_structure['distinct'] = True\n",
    "        col_label = next((lbl for lbl in [\"COLUMN_SECTOR\", \"COLUMN_INDUSTRY\", \"COLUMN_COUNTRY\"] if lbl in items_by_label), None)\n",
    "        if col_label and col_label in COLUMN_MAP:\n",
    "            parsed_structure['select_cols'] = [COLUMN_MAP[col_label]]\n",
    "        else:\n",
    "            parsed_structure['errors'].append(\"Could not determine column for listing values.\")\n",
    "            parsed_structure['select_cols'] = ['*'] \n",
    "\n",
    "    # 2. Top/Bottom N queries (Specific intent - HIGHER PRIORITY NOW)\n",
    "    elif has_ranking_modifier and has_cardinal: # Needs both modifier and number\n",
    "        parsed_structure['intent'] = 'find_top'\n",
    "        limit_item = items_by_label[\"CARDINAL\"][0]\n",
    "        try: parsed_structure['limit'] = int(limit_item['text'])\n",
    "        except ValueError: parsed_structure['errors'].append(f\"Could not parse limit value: {limit_item['text']}\")\n",
    "        order_col = \"Marketcap\"; order_dir = \"DESC\" # Defaults      \n",
    "        rank_item = items_by_label[\"RANKING_MODIFIER\"][0] \n",
    "        if has_ordering_keyword: # Check for explicit ordering column\n",
    "             order_kw_item = items_by_label[\"ORDERING_KEYWORD\"][0]\n",
    "             following_cols = [item for lbl in mentioned_columns_labels for item in items_by_label[lbl] if item['start_char'] > order_kw_item['end_char']]\n",
    "             following_cols.sort(key=lambda x: x['start_char'])\n",
    "             if following_cols and following_cols[0]['label'] in COLUMN_MAP: order_col = COLUMN_MAP[following_cols[0]['label']]\n",
    "        rank_token = doc.char_span(rank_item['start_char'], rank_item['end_char'], label=rank_item['label']) \n",
    "        # Corrected ASC check\n",
    "        if rank_token and len(rank_token) > 0 and rank_token[0].lemma_.lower() in [\"least\", \"low\", \"small\"]: order_dir = \"ASC\"\n",
    "        parsed_structure['order_by'] = {'column': order_col, 'direction': order_dir}\n",
    "        parsed_structure['select_cols'] = ['Security', order_col] \n",
    "            \n",
    "    # 3. Specific Column Lookup (Specific intent)\n",
    "    elif is_specific_col_req:\n",
    "         parsed_structure['intent'] = 'lookup_specific_column'\n",
    "         company_item = items_by_label.get(\"COMPANY_NAME\", items_by_label.get(\"ORG\", []))[0]\n",
    "         # ---> FIX: Ensure normalization <---\n",
    "         company_name_norm = normalize_term(company_item['text'], unique_companies_orig) \n",
    "         parsed_structure['filters'].append({ 'column': 'Security', 'operator': '=', 'value': company_name_norm })\n",
    "         col_label = mentioned_columns_labels[0]\n",
    "         if col_label in COLUMN_MAP:\n",
    "              parsed_structure['select_cols'] = [COLUMN_MAP[col_label]]\n",
    "         else:\n",
    "              parsed_structure['errors'].append(f\"Could not map requested column: {col_label}\")\n",
    "              parsed_structure['select_cols'] = ['*'] \n",
    "              \n",
    "    # 4. Simple Company Details Lookup (Specific intent)\n",
    "    elif has_company and not has_ranking_modifier and not has_comparison and not mentioned_columns_labels and len(identified_items) < 5: \n",
    "        parsed_structure['intent'] = 'lookup_details'\n",
    "        parsed_structure['select_cols'] = ['*'] \n",
    "        company_item = items_by_label.get(\"COMPANY_NAME\", items_by_label.get(\"ORG\", []))[0]\n",
    "        company_name_norm = normalize_term(company_item['text'], unique_companies_orig) \n",
    "        parsed_structure['filters'].append({ 'column': 'Security', 'operator': '=', 'value': company_name_norm })\n",
    "        \n",
    "    # 5. Default to Filtered List / General Ordering\n",
    "    else:\n",
    "        parsed_structure['intent'] = 'filter_list'\n",
    "        parsed_structure['select_cols'] = ['Security', 'Marketcap'] # Default columns\n",
    "        if has_ordering_keyword:\n",
    "             order_kw_item = items_by_label[\"ORDERING_KEYWORD\"][0]\n",
    "             following_cols = [item for lbl in mentioned_columns_labels for item in items_by_label[lbl] if item['start_char'] > order_kw_item['end_char']]\n",
    "             following_cols.sort(key=lambda x: x['start_char'])\n",
    "             if following_cols and following_cols[0]['label'] in COLUMN_MAP:\n",
    "                  order_col = COLUMN_MAP[following_cols[0]['label']]; order_dir = 'ASC' \n",
    "                  parsed_structure['order_by'] = {'column': order_col, 'direction': order_dir}\n",
    "\n",
    "    # --- Extract Filters (Common Logic - Ensure correct execution order) ---\n",
    "    added_filter_tuples = set() \n",
    "    # Initialize with filters potentially added by specific intents (like lookup_details)\n",
    "    for f in parsed_structure['filters']: \n",
    "        added_filter_tuples.add( (f['column'], f['operator'], str(f['value'])) )\n",
    "\n",
    "    # Only add MORE filters if the intent isn't a simple specific lookup\n",
    "    if parsed_structure['intent'] not in ['list_values', 'lookup_specific_column', 'lookup_details']:\n",
    "        \n",
    "        # Basic Filters (Country, Sector, Industry)\n",
    "        filter_labels_priority = [\"COUNTRY_TERM\", \"SECTOR_TERM\", \"INDUSTRY_TERM\"]\n",
    "        filter_labels_fallback = [\"GPE\"] \n",
    "        for item_label in filter_labels_priority + filter_labels_fallback:\n",
    "            if item_label in items_by_label and item_label in COLUMN_MAP:\n",
    "                db_column = COLUMN_MAP[item_label]\n",
    "                if not db_column: continue \n",
    "                for item in items_by_label[item_label]:\n",
    "                    filter_value_raw = item.get('mapped_value', item['text'])\n",
    "                    # ---> FIX: Ensure normalization consistently applied <---\n",
    "                    normalized_value = filter_value_raw \n",
    "                    if item_label in [\"SECTOR_TERM\", \"COLUMN_SECTOR\"]: normalized_value = normalize_term(filter_value_raw, unique_sectors_orig, sector_alias_mapping)\n",
    "                    elif item_label in [\"INDUSTRY_TERM\", \"COLUMN_INDUSTRY\"]: normalized_value = normalize_term(filter_value_raw, unique_industries_orig)\n",
    "                    elif item_label in [\"COUNTRY_TERM\", \"GPE\", \"COLUMN_COUNTRY\"]:\n",
    "                         if item.get('mapped_value') and item['mapped_value'].lower() in lc_countries_set: normalized_value = normalize_term(item['mapped_value'], unique_countries_orig)\n",
    "                         else: normalized_value = normalize_term(filter_value_raw, unique_countries_orig, country_mapping)\n",
    "                    elif item_label in [\"COMPANY_NAME\", \"ORG\"]: normalized_value = normalize_term(filter_value_raw, unique_companies_orig)\n",
    "                    \n",
    "                    # is_known check\n",
    "                    is_known = True\n",
    "                    if db_column == 'Sector' and normalized_value.lower() not in lc_sectors_set: is_known = False\n",
    "                    if db_column == 'Industry' and normalized_value.lower() not in lc_industries_set: is_known = False\n",
    "                    if db_column == 'Country' and normalized_value.lower() not in lc_countries_set: is_known = False\n",
    "                    if not is_known:\n",
    "                         parsed_structure['errors'].append(f\"Ignoring potential filter term '{item['text']}' for column '{db_column}' as it's not a known value.\")\n",
    "                         continue \n",
    "                         \n",
    "                    filter_tuple = (db_column, '=', str(normalized_value))\n",
    "                    if filter_tuple not in added_filter_tuples:\n",
    "                        parsed_structure['filters'].append({'column': db_column, 'operator': '=', 'value': normalized_value})\n",
    "                        added_filter_tuples.add(filter_tuple)\n",
    "\n",
    "    # --- Filter blocks that should run for MOST intents (incl. filter_list, find_top) ---\n",
    "    # Add Date, Price, MarketCap filters AFTER basic filters are processed\n",
    "    \n",
    "    # Date Filters \n",
    "    if has_comparison and has_year and has_date_keyword:\n",
    "        op_item = items_by_label[\"COMPARISON_OP\"][0]\n",
    "        year_item = items_by_label[\"YEAR_NUMBER\"][0]\n",
    "        sql_op = map_comparison_operator(op_item['text'])\n",
    "        year_val = year_item['text'] \n",
    "        if sql_op and year_val:\n",
    "             filter_column = \"Founded\" \n",
    "             filter_tuple = (filter_column, sql_op, str(year_val))\n",
    "             if filter_tuple not in added_filter_tuples:\n",
    "                  parsed_structure['filters'].append({'column': filter_column, 'operator': sql_op, 'value': year_val})\n",
    "                  added_filter_tuples.add(filter_tuple)\n",
    "             \n",
    "    # Price Filters \n",
    "    if has_comparison and has_price and has_price_keyword:\n",
    "        op_item = items_by_label[\"COMPARISON_OP\"][0]\n",
    "        price_item = items_by_label[\"PRICE_VALUE\"][0]\n",
    "        sql_op = map_comparison_operator(op_item['text'])\n",
    "        numeric_val = parse_monetary_value(price_item['text']) \n",
    "        if sql_op and numeric_val is not None:\n",
    "             filter_column = \"Stockprice\"\n",
    "             filter_tuple = (filter_column, sql_op, str(numeric_val))\n",
    "             if filter_tuple not in added_filter_tuples:\n",
    "                 parsed_structure['filters'].append({'column': filter_column, 'operator': sql_op, 'value': numeric_val})\n",
    "                 added_filter_tuples.add(filter_tuple)\n",
    "\n",
    "    # MarketCap Threshold Filters \n",
    "    if has_comparison and has_money and not has_price_keyword: \n",
    "         op_item = items_by_label[\"COMPARISON_OP\"][0]\n",
    "         val_item = items_by_label[\"MONEY_VALUE\"][0]\n",
    "         filter_column = \"Marketcap\" \n",
    "         sql_op = map_comparison_operator(op_item['text'])\n",
    "         numeric_val = parse_monetary_value(val_item['text'])\n",
    "         if sql_op and numeric_val is not None:\n",
    "              filter_tuple = (filter_column, sql_op, str(numeric_val))\n",
    "              if filter_tuple not in added_filter_tuples:\n",
    "                   parsed_structure['filters'].append({'column': filter_column, 'operator': sql_op, 'value': numeric_val})\n",
    "                   added_filter_tuples.add(filter_tuple)\n",
    "         # Note: We still don't have a fix for \"1B.\" -> MONEY_VALUE, so this block won't catch that.\n",
    "\n",
    "    # --- Refine Select Columns (Enhanced) ---\n",
    "    if parsed_structure['intent'] in ['filter_list', 'find_top']:\n",
    "        # ... (Keep existing logic) ...\n",
    "        current_cols = set(parsed_structure['select_cols']) \n",
    "        explicit_cols = set()\n",
    "        explicit_col_request = False\n",
    "        for col_label in mentioned_columns_labels:\n",
    "             if col_label in COLUMN_MAP and COLUMN_MAP[col_label]:\n",
    "                  explicit_cols.add(COLUMN_MAP[col_label])\n",
    "                  explicit_col_request = True\n",
    "        if explicit_col_request:\n",
    "             final_cols = ['Security'] + sorted(list(explicit_cols))\n",
    "             parsed_structure['select_cols'] = list(dict.fromkeys(final_cols)) \n",
    "             \n",
    "    return parsed_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e604ea",
   "metadata": {},
   "source": [
    "### 3.3. Definition: SQL Generation Function & Helpers\n",
    "Define the `generate_sql_from_structure` function and its `format_sql_value` helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SQL Formatting Helper ---\n",
    "def format_sql_value(value):\n",
    "    \"\"\"Formats a Python value for safe inclusion in an SQL query.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        escaped_value = value.replace(\"'\", \"''\")\n",
    "        return f\"'{escaped_value}'\"\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return str(value)\n",
    "    elif value is None:\n",
    "        return \"NULL\"\n",
    "    elif isinstance(value, bool):\n",
    "        return \"1\" if value else \"0\"\n",
    "    else:\n",
    "        escaped_value = str(value).replace(\"'\", \"''\")\n",
    "        # Removing the print warning from here, let the parsing step handle warnings\n",
    "        # print(f\"Warning: Formatting unexpected type {type(value)} as string: {value}\") \n",
    "        return f\"'{escaped_value}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00aae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SQL Generation Function ---\n",
    "def generate_sql_from_structure(parsed_structure):\n",
    "    \"\"\"\n",
    "    Translates the structured dictionary from Step 6 into a valid SQLite query,\n",
    "    and returns any parsing errors encountered in Step 6.\n",
    "\n",
    "    Args:\n",
    "        parsed_structure (dict): The dictionary output from parse_query_structure_refactored.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (sql_query_string, list_of_parsing_errors)\n",
    "    \"\"\"\n",
    "    parsing_errors = parsed_structure.get('errors', []) # Get errors from input structure\n",
    "    sql_query = \"-- No SQL generated due to invalid input.\" # Default SQL if structure is bad\n",
    "\n",
    "    if not parsed_structure or not parsed_structure.get('intent'):\n",
    "        parsing_errors.append(\"Invalid or unparsed query structure provided.\")\n",
    "        return (sql_query, parsing_errors)\n",
    "\n",
    "    # --- Generate SQL based on structure (same logic as before) ---\n",
    "    try:\n",
    "        select_parts = []\n",
    "        from_clause = 'FROM \"companies\"' \n",
    "        where_clause = None\n",
    "        orderby_clause = None\n",
    "        limit_clause = None\n",
    "\n",
    "        select_prefix = \"SELECT DISTINCT\" if parsed_structure.get('distinct', False) else \"SELECT\"\n",
    "        select_cols = parsed_structure.get('select_cols')\n",
    "        \n",
    "        if not select_cols: select_parts = [\"*\"]\n",
    "        elif select_cols == ['*']: select_parts = [\"*\"]\n",
    "        else: select_parts = [f'\"{col}\"' for col in select_cols]\n",
    "        select_clause = f\"{select_prefix} {', '.join(select_parts)}\"\n",
    "\n",
    "        filters = parsed_structure.get('filters')\n",
    "        if filters:\n",
    "            conditions = []\n",
    "            for f in filters:\n",
    "                col, op, val = f.get('column'), f.get('operator', '='), f.get('value')\n",
    "                if not col: continue\n",
    "                quoted_col = f'\"{col}\"'\n",
    "                if val is None:\n",
    "                     if op == '=': conditions.append(f\"{quoted_col} IS NULL\")\n",
    "                     elif op in ['!=', '<>']: conditions.append(f\"{quoted_col} IS NOT NULL\")\n",
    "                     else: continue \n",
    "                else:\n",
    "                     formatted_val = format_sql_value(val)\n",
    "                     conditions.append(f\"{quoted_col} {op} {formatted_val}\")\n",
    "            if conditions: where_clause = \"WHERE \" + \" AND \".join(conditions)\n",
    "\n",
    "        order_by = parsed_structure.get('order_by')\n",
    "        if order_by:\n",
    "            col, direction = order_by.get('column'), order_by.get('direction', 'ASC').upper()\n",
    "            if col and direction in ['ASC', 'DESC']: orderby_clause = f'ORDER BY \"{col}\" {direction}'\n",
    "\n",
    "        limit = parsed_structure.get('limit')\n",
    "        if limit is not None:\n",
    "            try:\n",
    "                limit_val = int(limit)\n",
    "                if limit_val > 0: limit_clause = f\"LIMIT {limit_val}\"\n",
    "            except (ValueError, TypeError): pass # Ignore invalid limit\n",
    "\n",
    "        query_parts = [select_clause, from_clause, where_clause, orderby_clause, limit_clause]\n",
    "        sql_query = \" \".join(filter(None, query_parts)).strip() + \";\"\n",
    "\n",
    "    except Exception as e:\n",
    "         # Catch potential errors during SQL string construction itself\n",
    "         parsing_errors.append(f\"Error during SQL generation: {e}\")\n",
    "         sql_query = \"-- Error occurred during SQL generation.\"\n",
    "\n",
    "    # --- Return both the generated SQL and the list of parsing errors ---\n",
    "    return (sql_query, parsing_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fe074",
   "metadata": {},
   "source": [
    "## 4. NLP Pipeline Execution & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fc081",
   "metadata": {},
   "source": [
    "### 4.1. Load Gazetteers & Apply NER Pipeline\n",
    "Load terms from the database, add patterns to the NLP components, and process test queries to extract entities. Store results for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd2b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = {}\n",
    "identified_items_store = {}\n",
    "test_queries = [] # Define the final list of test queries here\n",
    "\n",
    "if nlp and matcher and ruler:\n",
    "    # 1. Load Gazetteers\n",
    "    print(\"\\n--- Loading Gazetteers from Database ---\")\n",
    "    lc_companies, unique_companies_orig = load_terms_from_db(db_file, table_name, 'Security')\n",
    "    lc_sectors, unique_sectors_orig = load_terms_from_db(db_file, table_name, 'Sector')\n",
    "    lc_countries, unique_countries_orig = load_terms_from_db(db_file, table_name, 'Country')\n",
    "    lc_industries, unique_industries_orig = load_terms_from_db(db_file, table_name, 'Industry')\n",
    "    \n",
    "    # Combine terms for matching (needed if PhraseMatcher uses combined lists)\n",
    "    all_country_terms_lc = set(lc_countries) | set(country_mapping.keys())\n",
    "    all_sector_terms_lc = set(lc_sectors) | set(sector_alias_mapping.keys())\n",
    "    all_industry_terms_lc = set(lc_industries) \n",
    "    all_company_terms_lc = set(lc_companies)\n",
    "\n",
    "    # 2. Add Patterns to Matcher\n",
    "    print(\"\\n--- Adding PhraseMatcher Patterns ---\")\n",
    "    # Only add if not empty and len > 1\n",
    "    if all_company_terms_lc: matcher.add(\"COMPANY_NAME\", [doc for doc in nlp.pipe(all_company_terms_lc) if len(doc) > 1])\n",
    "    if all_sector_terms_lc: matcher.add(\"SECTOR_TERM\", [doc for doc in nlp.pipe(all_sector_terms_lc) if len(doc) > 1])\n",
    "    # Country patterns\n",
    "    for term in lc_countries:\n",
    "        if len(nlp(term)) == 1:  # Single token countries\n",
    "            patterns.append({\"label\": \"COUNTRY_TERM\", \"pattern\": [{\"LOWER\": term}], \"id\": term})\n",
    "\n",
    "    # Country aliases  \n",
    "    for alias, canonical in country_mapping.items():\n",
    "        patterns.append({\"label\": \"COUNTRY_TERM\", \"pattern\": [{\"LOWER\": alias}], \"id\": canonical})\n",
    "\n",
    "    # Sector patterns - Carefully add single tokens if needed\n",
    "    # for term in lc_sectors:\n",
    "    #     if len(nlp(term)) == 1:\n",
    "    #         patterns.append({\"label\": \"SECTOR_TERM\", \"pattern\": [{\"LOWER\": term}], \"id\": term})\n",
    "\n",
    "    # Sector aliases\n",
    "    for alias, canonical in sector_alias_mapping.items():\n",
    "        patterns.append({\"label\": \"SECTOR_TERM\", \"pattern\": [{\"LOWER\": alias}], \"id\": canonical})\n",
    "        \n",
    "    # Add multi-word sector pattern for \"info tech\" explicitly\n",
    "    patterns.append({\"label\": \"SECTOR_TERM\", \"pattern\": [{\"LOWER\": \"info\"}, {\"LOWER\": \"tech\"}], \"id\": \"information technology\"})\n",
    "\n",
    "    # Industry patterns\n",
    "    for term in lc_industries:\n",
    "        if len(nlp(term)) == 1:\n",
    "            patterns.append({\"label\": \"INDUSTRY_TERM\", \"pattern\": [{\"LOWER\": term}], \"id\": term})\n",
    "\n",
    "    # Company patterns\n",
    "    for term in lc_companies:\n",
    "        if len(nlp(term)) == 1:\n",
    "            patterns.append({\"label\": \"COMPANY_NAME\", \"pattern\": [{\"LOWER\": term}], \"id\": term})\n",
    "\n",
    "    # Fix money value pattern to better handle \"1B.\" cases\n",
    "    patterns.append({\"label\": \"MONEY_VALUE\", \"pattern\": [\n",
    "        {\"LIKE_NUM\": True}, \n",
    "        {\"LOWER\": {\"IN\": [\"b\", \"bn\", \"billion\", \"m\", \"mn\", \"million\", \"t\", \"tn\", \"trillion\"]}},\n",
    "        {\"TEXT\": \".\", \"OP\": \"?\"} # Optional period\n",
    "    ]})\n",
    "    \n",
    "    patterns.extend(custom_patterns)\n",
    "    \n",
    "    # Add all patterns to the ruler\n",
    "    ruler.add_patterns(patterns)\n",
    "\n",
    "    # --- Process Example Queries ---\n",
    "    test_queries = [\n",
    "        \"Top 10 most valuable American IT companies.\",\n",
    "        \"Show details for Apple Inc.\",\n",
    "        \"Every French company valued over 1B.\",\n",
    "        \"List all sectors available.\",\n",
    "        \"What is the market cap of Microsoft?\",\n",
    "        \"Which German companies are in the Health Care sector?\", \n",
    "        \"Find companies worth more than 500 billion dollars\", \n",
    "        \"Show financials for Tesla\", \n",
    "        \"list info tech companies in the US\", \n",
    "        \"Lowest 5 market cap companies in France\", \n",
    "        \"Show country and sector for companies in Spain\",\n",
    "        \"What sector is Apple in?\",\n",
    "        \"Show the founding date for Microsoft.\",\n",
    "        \"List stock prices for IT companies.\",\n",
    "        \"Companies founded after 1990.\",\n",
    "        \"Companies with stock price over $500.\",\n",
    "        \"USA companies in IT founded before 2000.\",\n",
    "        \"List companies ordered by founding date.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Analyzing Queries with Refactored Enhanced NER ---\")\n",
    "    processed_docs = {}\n",
    "    identified_items_store = {}\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n--- Analyzing Query: '{query}' ---\")\n",
    "        doc = nlp(query)\n",
    "        processed_docs[query] = doc \n",
    "        phrase_matches = matcher(doc)\n",
    "\n",
    "        # --- Collect Entities/Matches (Revised Logic v2) ---\n",
    "        # Prioritize pipeline entities (Ruler runs first with overwrite=True)\n",
    "        found_items = []\n",
    "        pipeline_ents = {} \n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            span_key = (ent.start_char, ent.end_char)\n",
    "            ent_id = ent.ent_id_ if ent.ent_id_ else None \n",
    "            \n",
    "            # Attempt normalization based on label and ID/text\n",
    "            mapped_value = ent.text # Default\n",
    "            try:\n",
    "                if ent.label_ == \"COUNTRY_TERM\":\n",
    "                     # Use ID if it's a known canonical name, else try mapping the text\n",
    "                    if ent_id and ent_id in lc_countries: mapped_value = ent_id \n",
    "                    elif ent.text.lower() in country_mapping: mapped_value = country_mapping[ent.text.lower()]\n",
    "                elif ent.label_ == \"SECTOR_TERM\":\n",
    "                    if ent_id and ent_id in lc_sectors: mapped_value = ent_id\n",
    "                    elif ent.text.lower() in sector_alias_mapping: mapped_value = sector_alias_mapping[ent.text.lower()]\n",
    "                # Add Industry/Company normalization if needed using ent_id and lc_ lists\n",
    "                \n",
    "            except Exception as e:\n",
    "                 print(f\"  Warning: Error during mapping/normalization for '{ent.text}' ({ent.label_}): {e}\")\n",
    "\n",
    "            # Store the entity, allowing overwrite based on Ruler's priority\n",
    "            pipeline_ents[span_key] = {\n",
    "                \"text\": ent.text, \"label\": ent.label_, \"ent_id\": ent_id,\n",
    "                \"mapped_value\": mapped_value, \n",
    "                \"start_char\": ent.start_char, \"end_char\": ent.end_char,\n",
    "                \"source\": \"Pipeline (Ruler/NER)\" \n",
    "            }\n",
    "\n",
    "        found_items.extend(pipeline_ents.values())\n",
    "\n",
    "        # Add non-overlapping PhraseMatcher results\n",
    "        pipeline_spans = {(item['start_char'], item['end_char']) for item in found_items}\n",
    "        for match_id, start, end in phrase_matches:\n",
    "            span = doc[start:end]\n",
    "            span_chars = (span.start_char, span.end_char)\n",
    "            if span_chars not in pipeline_spans:\n",
    "                label = nlp.vocab.strings[match_id]\n",
    "                term_text = span.text\n",
    "                mapped_value = term_text # Basic mapping for PhraseMatcher results if needed\n",
    "                if label == \"COUNTRY_TERM\" and span.text.lower() in country_mapping: mapped_value = country_mapping[span.text.lower()]\n",
    "                elif label == \"SECTOR_TERM\" and span.text.lower() in sector_alias_mapping: mapped_value = sector_alias_mapping[span.text.lower()]\n",
    "                \n",
    "                found_items.append({\n",
    "                    \"text\": term_text, \"label\": label, \"mapped_value\": mapped_value,\n",
    "                    \"ent_id\": None, \n",
    "                    \"start_char\": span.start_char, \"end_char\": span.end_char,\n",
    "                    \"source\": \"PhraseMatcher\"\n",
    "                })\n",
    "\n",
    "        found_items.sort(key=lambda item: item['start_char'])\n",
    "        identified_items_store[query] = found_items \n",
    "\n",
    "        # Display Results\n",
    "        print(\"\\nIdentified Entities & Terms:\")\n",
    "        if found_items:\n",
    "            for item in found_items:\n",
    "                details = f\"- Text: '{item['text']}', Label: {item['label']}, Source: {item['source']}\"\n",
    "                if item['mapped_value'] != item['text']: details += f\" (Mapped: '{item['mapped_value']}')\"\n",
    "                if item['ent_id']: details += f\" (ID: {item['ent_id']})\"\n",
    "                print(details)\n",
    "        else: print(\"- No specific entities or terms identified.\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\nSkipping analysis because the spaCy model could not be loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
